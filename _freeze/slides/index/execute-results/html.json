{
  "hash": "111695fc5f2c318494d0a5770a748909",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"proPower: Prospective power analyses for frequentist regression models\"\nauthor: \"Dr Stefano Coretta\"\ninstitute: \"University of Edinburgh\"\ndate: \"2025/03/26\"\nformat: \n  mono-light-revealjs:\n    theme: [default, \"custom.scss\"]\n    history: false\n    fig-align: center\nfilters:\n  - tachyonsextra\nexecute: \n  echo: false\n  fig-align: center\nbibliography: references.bib\n---\n\n\n\n\n\n## Frequentist statistics\n\n::: box-note\n-   *P*-values: $P(d|H)$, probability of the difference $d$ (or a more extreme one) given the hypothesis $H$.\n\n-   Neyman-Pearson NHST:\n\n    -   $H$ is the \"null hypothesis\", to be rejected.\n\n    -   $\\alpha$ level: $p \\ge \\alpha$ non-significant difference, $p < \\alpha$ significant difference.\n\n    -   $\\beta$ level: statistical power, probability of detecting a significant difference when there is one.\n\n-   Null Ritual [@gigerenzer2004; @gigerenzer2004a; @gigerenzer2018]:\n\n    -   $H$ is a \"nil hypothesis\": $d = 0$, to be rejected.\n\n    -   $\\alpha = 0.05$\n\n    -   $\\beta = 0.8$\n:::\n\n## Statistical power\n\n::: box-tip\n-   Statistical power ($\\beta$ level) is affected by sample size.\n\n-   One must determine the sample size needed to achieve the desired statistical power. In Null Ritualistic statistics this is 0.8 (or 80% power).\n\n-   Prospective power analysis.\n:::\n\n. . .\n\n::: box-note\nTwo examples\n\n-   Effect of consonant voicing on vowel duration in mono- vs disyllabic words.\n\n-   Accuracy in a lexical decision task to investigate shallow morphological representation in L2 speakers.\n:::\n\n## Example 1: vowel duration\n\n::: box-note\n-   Data from @coretta2019f, @coretta2019i.\n\n-   \"Voicing effect\" in Manchester English.\n\n-   Mono- and di-syllabic nonce words in frame sentences.\n\n-   Is the voicing effect the same or different in mono- vs disyllabic words?\n:::\n\n. . .\n\n::: box-tip\n-   Use \"pilot\" data to determine sample size.\n:::\n\n## Example 1: the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\neng_s <- readRDS(\"data/eng_s.rds\")\neng_s |> select(speaker, num_syl, word, voicing, v1_duration, speech_rate_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 720 × 6\n   speaker num_syl word   voicing   v1_duration speech_rate_c\n   <fct>   <fct>   <fct>  <fct>           <dbl>         <dbl>\n 1 en02    di      teegus voiced          117.         -0.650\n 2 en02    mono    teek   voiceless        95.0        -0.908\n 3 en02    mono    teek   voiceless       106.         -0.900\n 4 en02    di      tergus voiced          190.         -0.750\n 5 en02    di      targus voiced          195.         -0.383\n 6 en02    di      teepus voiceless       113.         -0.655\n 7 en02    mono    terb   voiced          203.         -0.823\n 8 en02    mono    tarp   voiceless       217.         -0.534\n 9 en02    di      teegus voiced          120.         -0.571\n10 en02    di      terpus voiceless       147.         -0.665\n# ℹ 710 more rows\n```\n\n\n:::\n:::\n\n\n\n## Effect of voicing on vowel duration\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/eng-s-plot-1.png){width=960}\n:::\n:::\n\n\n\n## Prospective power analysis: overview\n\n::: box-tip\n1.  Determine Smallest Meaningful Effect Size (SMES).\n\n2.  Run regression model on pilot or simulated data.\n\n3.  Extend model with `simr::extend()` and set coefficient to SMES.\n\n4.  Run power calculation with `simr::powerCurve()`.\n:::\n\n## Smallest Meaningful Effect Size\n\n::: box-note\n-   The minimum standard deviation for measurements of vowel duration is between 2 to 5 ms [@allen1978].\n\n-   @nowak2006 finds an effect of voicing of 4.5 ms.\n\n-   Let's use an SMES of 5 ms. What does this correspond to as a difference ratio?\n:::\n\n## Smallest Meaningful Effect Size (ratios)\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nx <- seq(50, 300, by = 10)\ny <- x + 5\n\nggplot() +\n  aes(x, y/x) +\n  geom_point(size = 2) +\n  labs(\n    x = \"Vowel duration\",\n    y = \"Difference ratio\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/smes-1.png){width=384}\n:::\n:::\n\n\n\n## Run regression model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndur_lm <- lmer(\n  log(v1_duration) ~\n    voicing + num_syl + voicing:num_syl + speech_rate_c +\n    (voicing | speaker),\n  data = eng_s\n)\n\nsummary(dur_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: \nlog(v1_duration) ~ voicing + num_syl + voicing:num_syl + speech_rate_c +  \n    (voicing | speaker)\n   Data: eng_s\n\nREML criterion at convergence: -230\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3800 -0.5941  0.1648  0.6624  3.1393 \n\nRandom effects:\n Groups   Name          Variance  Std.Dev. Corr\n speaker  (Intercept)   0.0181967 0.13490      \n          voicingvoiced 0.0004701 0.02168  0.37\n Residual               0.0397878 0.19947      \nNumber of obs: 720, groups:  speaker, 6\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)                4.66722    0.05705  81.816\nvoicingvoiced              0.17276    0.02281   7.573\nnum_sylmono                0.08260    0.02308   3.579\nspeech_rate_c             -0.22902    0.02985  -7.673\nvoicingvoiced:num_sylmono  0.05501    0.02974   1.850\n\nCorrelation of Fixed Effects:\n            (Intr) vcngvc nm_syl spch__\nvoicingvocd -0.029                     \nnum_sylmono -0.163  0.417              \nspeech_rt_c  0.011 -0.006  0.412       \nvcngvcd:nm_  0.130 -0.652 -0.647 -0.006\n```\n\n\n:::\n:::\n\n\n\n## Extend model and set SMES\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndur_lm_ext <- extend(dur_lm, along = \"speaker\", n = 20)\n\nfixef(dur_lm_ext)[\"voicingvoiced:num_sylmono\"] <- 0.03\nfixef(dur_lm_ext)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              (Intercept)             voicingvoiced               num_sylmono \n               4.66722417                0.17276388                0.08260109 \n            speech_rate_c voicingvoiced:num_sylmono \n              -0.22902490                0.03000000 \n```\n\n\n:::\n:::\n\n\n\n## Run power analysis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(simr)\n\ndur_lm_pow <- powerCurve(\n  dur_lm_ext,\n  fixed(\"voicing:num_syl\"),\n  along = \"speaker\",\n  breaks = c(50, 70, 100)\n)\n\nprint(dur_lm_pow)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nPower for predictor 'voicing:num_syl', (95% confidence interval),\nby number of levels in speaker:\n     50: 83.30% (80.84, 85.56) - 6000 rows\n     70: 92.90% (91.13, 94.41) - 8400 rows\n    100: 98.90% (98.04, 99.45) - 12000 rows\n\nTime elapsed: 1 h 12 m 47 s\n```\n\n\n:::\n:::\n\n\n\n## Power curve plot\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(dur_lm_pow)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/power-curve-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Example 2: accuracy\n\n::: box-note\n-   Data from @song2020.\n\n-   Shallow morphological representation in L2 speakers.\n\n-   **Lexical decision task**: Is the word a real English word or not?\n\n-   **Priming paradigm**:\n\n    -   **Prime**: *prolong* (unrelated), *unkind* (constituent), *kindness* (non-constituent).\n\n    -   **Target**: *unkindness* (*\\[un-kind\\]-ness*, not *un-\\[kind-ness\\]*).\n\n-   Constituent vs non-constituent accuracy in L2 speakers.\n:::\n\n## Example 2: the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshallow_s <- readRDS(\"data/shallow_s.rds\")\nshallow_s\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 180 × 11\n   ID    Group List  Target         ACC    RT logRT Critical_Filler Word_Nonword\n   <chr> <chr> <chr> <chr>        <dbl> <dbl> <dbl> <chr>           <chr>       \n 1 L2_36 L2    D     unholiness       1   878  6.78 Critical        Word        \n 2 L2_36 L2    D     rehydratable     1  1263  7.14 Critical        Word        \n 3 L2_36 L2    D     unclearness      1  1296  7.17 Critical        Word        \n 4 L2_36 L2    D     resellable       1  1344  7.20 Critical        Word        \n 5 L2_36 L2    D     unsharpness      0  1192  7.08 Critical        Word        \n 6 L2_36 L2    D     reattachable     1   969  6.88 Critical        Word        \n 7 L2_36 L2    D     uncleverness     0   835  6.73 Critical        Word        \n 8 L2_36 L2    D     unwariness       1  1028  6.94 Critical        Word        \n 9 L2_36 L2    D     reclosable       1  1028  6.94 Critical        Word        \n10 L2_36 L2    D     reusable         1   814  6.70 Critical        Word        \n# ℹ 170 more rows\n# ℹ 2 more variables: Relation_type <chr>, Branching <chr>\n```\n\n\n:::\n:::\n\n\n\n## Accuracy by relation type\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/shall-plot-1.png){width=960}\n:::\n:::\n\n\n\n## Smallest Meaningful Effect Size (log-odds)\n\n::: box-note\n-   I have no idea...\n\n-   Say -0.2 (log-odds).\n:::\n\n## Smallest Meaningful Effect Size (probability)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/prob-diff-1.png){width=960}\n:::\n:::\n\n\n\n## Run regression model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsha_lm <- glmer(\n  ACC ~ Relation_type + (Relation_type | ID),\n  family = binomial,\n  data = shallow_s\n)\n\nsummary(sha_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: ACC ~ Relation_type + (Relation_type | ID)\n   Data: shallow_s\n\n     AIC      BIC   logLik deviance df.resid \n   243.9    272.7   -113.0    225.9      171 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8220 -1.1732  0.5994  0.7376  0.9667 \n\nRandom effects:\n Groups Name                        Variance Std.Dev. Corr       \n ID     (Intercept)                 0.5606   0.7487              \n        Relation_typeNonConstituent 0.1617   0.4022   -1.00      \n        Relation_typeUnrelated      0.2396   0.4895   -1.00  1.00\nNumber of obs: 180, groups:  ID, 10\n\nFixed effects:\n                              Estimate Std. Error z value Pr(>|z|)  \n(Intercept)                  0.8699404  0.3904283   2.228   0.0259 *\nRelation_typeNonConstituent  0.0008542  0.4401554   0.002   0.9985  \nRelation_typeUnrelated      -0.3867280  0.4379963  -0.883   0.3773  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Rlt_NC\nRltn_typNnC -0.730       \nRltn_typUnr -0.777  0.596\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00637234 (tol = 0.002, component 1)\n```\n\n\n:::\n:::\n\n\n\n## Run regression model II\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsha_lm_2 <- glmer(\n  ACC ~ Relation_type + (1 | ID),\n  family = binomial,\n  data = shallow_s\n)\n\nsummary(sha_lm_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: ACC ~ Relation_type + (1 | ID)\n   Data: shallow_s\n\n     AIC      BIC   logLik deviance df.resid \n   235.1    247.8   -113.5    227.1      176 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7702 -1.2415  0.6161  0.7087  0.9373 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.14     0.3742  \nNumber of obs: 180, groups:  ID, 10\n\nFixed effects:\n                            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)                  0.79468    0.30687   2.590  0.00961 **\nRelation_typeNonConstituent  0.08048    0.40084   0.201  0.84086   \nRelation_typeUnrelated      -0.30295    0.38990  -0.777  0.43716   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Rlt_NC\nRltn_typNnC -0.643       \nRltn_typUnr -0.665  0.506\n```\n\n\n:::\n:::\n\n\n\n## Run power analysis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsha_lm_ext <- extend(sha_lm_2), along = \"ID\", n = 1000)\nfixef(sha_lm_ext)[\"Relation_typeNonConstituent\"] <- -0.2\n\nsha_lm_pow <- powerCurve(sha_lm_ext, fixed(\"Relation_typeNonConstituent\"), along = \"ID\")\nprint(sha_lm_pow)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nPower for predictor 'Relation_typeNonConstituent', (95% confidence interval),\nby largest value of ID:\n      3:  5.00% ( 3.73,  6.54) - 54 rows\n    114: 37.50% (34.49, 40.58) - 2052 rows\n    225: 67.60% (64.60, 70.50) - 4050 rows\n    335: 83.30% (80.84, 85.56) - 6030 rows\n    446: 92.40% (90.58, 93.97) - 8028 rows\n    557: 95.70% (94.25, 96.87) - 10026 rows\n    668: 98.30% (97.29, 99.01) - 12024 rows\n    778: 99.20% (98.43, 99.65) - 14004 rows\n    889: 99.70% (99.13, 99.94) - 16002 rows\n   1000: 100.0% (99.63, 100.0) - 18000 rows\n\nTime elapsed: 1 h 29 m 48 s\n```\n\n\n:::\n:::\n\n\n\n## Power curve plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sha_lm_pow)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/sha-pow-curve-1.png){width=960}\n:::\n:::\n\n\n\n## Summary\n\n::: box-note\n1.  Determine Smallest Meaningful Effect Size (SMES).\n\n2.  Run regression model on pilot or simulated data.\n\n3.  Extend model with `simr::extend()` and set coefficient to SMES.\n\n4.  Run power calculation with `simr::powerCurve()`.\n:::\n\n. . .\n\n::: box-tip\n**Questions?**\n:::\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}